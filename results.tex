\section{Experimental Setup and Evaluation}
To evaluate the performance of ImageHarbour, we conducted experiments comparing it with three other image retrieval methods: local disk, private registry, and Docker Hub. The experiments were designed to measure the latency of image retrieval for different image sizes.

\subsection{Experimental Setup}
The experimental setup consisted of a cluster of machines (\href{https://arc.net/l/quote/gpedzjix}{c6525-25g}) connected via a high-speed network (25Gbps links). Each machine was equipped with a 16 core AMD 7302P processor, 128 GB of RAM, a dual-port ConnectX-5 25Gbps NIC and a 480 GB SATA SSD. The machines were running Ubuntu 22.04 LTS and had Docker installed. For client to image server communication, we employed a low-latency datacenter RPC framework \cite{erpc}. For our experiments, we used a single memory server with 20GB of available memory for caching images. 

We set up the following four systems for comparison:
\begin{enumerate}
    \itemsep0em
    \item \textbf{Local Disk}: In this setup, the required Docker image was already present on the local disk of the host machine. 
    \item \textbf{Local Registry}: We deployed a private Docker registry within the same cluster as the client machines. The required Docker images were hosted on this local registry.
    \item \textbf{Docker Hub}: The client machines fetched the required Docker images directly from Docker Hub, the public Docker image registry.
    \item \textbf{ImageHarbour}: Our proposed ImageHarbour system was deployed on the cluster, with the control plane, memory pool, and client nodes set up according to the architecture described in Section~\ref{sec:system_design}.
\end{enumerate}

In each system, we repeatedly measure the time taken to retrieve and setup a Docker image on the client machine from the respective sources. This includes the time taken to perform the \texttt{docker load} operation to build the image from an image file. The docker cache is pruned after each iteration. Each experiment runs for 3 minutes. To evaluate the performance of each system under different image sizes, we selected four representative Docker images:

\begin{enumerate}
    \itemsep0em
    \item \textbf{hello-world}: A lightweight Docker image with an uncompressed size of 25KB. This image represents small-sized images commonly used for testing and simple applications.
    \item \textbf{alpine}: A popular lightweight Linux distribution image with an uncompressed size of 7.32MB. Alpine is widely used as a base image for containerized applications due to its small footprint.
    \item \textbf{debian}: A larger Docker image based on the Debian Linux distribution, with an uncompressed size of 116MB. This image represents more substantial application images that include a full-fledged operating system and additional dependencies.
    \item \textbf{pytorch}: A very large Docker image containing the PyTorch deep learning framework, with an uncompressed size of 7.2GB. This image represents heavyweight images used for machine learning and data science applications.
\end{enumerate}

In the following subsections, we present the results of our experiments and discuss the performance of ImageHarbour compared to the other image retrieval methods.

\subsection{Results}

\begin{table}[h]
    \centering
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{lllll}
    \Xhline{2\arrayrulewidth}
    \textbf{Image} & \textbf{Local Disk (ms)} & \textbf{Local Registry (ms)} & \textbf{DockerHub (ms)} & \textbf{ImageHarbour (ms)} \\ \hline
    hello-world & 130.35 (8x) & 150.42 (9x) & 1226.20 (74x) & 16.60 \\ 
    alpine & 205.04 (2.31x) & 205.04 (2.31x) & 1528.01 (17x) & 88.56 \\ 
    debian & 1387.90 (1.12x) & 1387.75 (1.12x) & 3167.98 (2.5x) & 1232.25 \\ 
    pytorch & 46461.74 (1.18x) & 48982.70 (1.18x) & 99647.49 (2.4x) & 41173 \\
    \Xhline{2\arrayrulewidth}
    \end{tabular}
    }
    \caption{Image retrieval times (in milliseconds) for different images. The numbers in the bracket for Local Disk, Local Registry, and DockerHub represent the latency slowdown factor as compared to ImageHarbour.}
    \label{tab:image_retrieval_times}
\end{table}


\begin{table}[h]
    \centering
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{lll}
    \Xhline{2\arrayrulewidth}
    \textbf{Image} & \textbf{Fetch + Store uncompressed image ($\mu$s)} & \textbf{SCP compressed image ($\mu$s)} \\ \hline
    hello-world & 61.7055 (0.4\%) & 217564 (3525x) \\
    alpine  & 6662.96 (7.5\%) & 222916 (33x) \\
    debian & 97074.9 (7.9\%) & 338438 (3.5x) \\
    pytorch & 5721700 (14\%) & 8470070 (1.5x) \\
    \Xhline{2\arrayrulewidth}
    \end{tabular}
    }
    \caption{Table showing the time to just fetch and store the uncompressed image in ImageHarbour (without doing \texttt{docker load}) and time to SCP the compressed image. The numbers in the bracket for fetch and store represent the percentage of the total time taken to fetch and store the image. The numbers in the bracket for SCP represent the latency slowdown factor as compared to fetch and store for ImageHarbour.}
    \label{tab:scp}
\end{table}


Table \ref{tab:image_retrieval_times} shows the image retreival times for the different workload images. For small images such as hello-world, we observe 8-9x lower latency in ImageHarbour as opposed to using a local disk or a local image registry and 74x lower latency in comparision to using DockerHub to fetch the images. For larger images, such as pytorch, we obtain a 1.18x latency benefit over using a local image repository or disk and a 2.4x latency benefit over using DockerHub. This shows that ImageHarbour performs better or at par with other approaches for image retreival while efficiently utilizing the stranded memory in the cluster instead of dedicated CPU, disk and memory resources.

To better understand these numbers, we also measured the time taken to just fetch and store the \textit{uncompressed} image in ImageHarbour (without doing \texttt{docker load}). Column 2 in Table \ref{tab:scp} shows that the fetch and store time is a small fraction (varying from 0.4 - 14\%) of the total time to retreive the image. This shows that the majority of the latency comes from the \texttt{docker load} operation which hides the latency advantages in fetching and storing the image in ImageHarbour. To demonstrate this, we also measured the time taken to SCP a \textit{compressed} image across two nodes in the same cluster. 
Since the image is compressed and fetched across a very fast local network, this represents the absolute best case image fetch and store times possible without using ImageHarbour (comparable to using a local registry and better than fetching over the internet). Column 3 in Table \ref{tab:scp} shows that even in the most optimal scenario, this time is significantly larger than the fetch and store time in ImageHarbour (1.5x - 3525x higher latencies). This factor along with the efficient utilization of stranded memory, disk and CPU resources demonstrate the true benefits of using ImageHarbour.  

\subparagraph*{Note on Scalability:} We also conducted scalability experiments to evaluate the performance of ImageHarbour with increasing number of client nodes. We observed minimal to no latency degradation with upto 10 client nodes. The bottleneck in this design is the NIC card on the memory server which serves all the read requests. This bottleneck can be easily avoided by a better design where popular images are duplicated across multiple memory servers and the client nodes are load balanced across these memory servers. 

